{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    # Draw face mesh\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.face_landmarks, \n",
    "            mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "        )\n",
    "    \n",
    "    # Draw pose connections\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_holistic.POSE_CONNECTIONS\n",
    "        )\n",
    "    \n",
    "    # Draw hands\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.left_hand_landmarks, \n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )\n",
    "    \n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.right_hand_landmarks, \n",
    "            mp_holistic.HAND_CONNECTIONS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections with error checking\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.face_landmarks, \n",
    "            mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "        )\n",
    "    \n",
    "    # Draw pose connections\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.pose_landmarks, \n",
    "            mp_holistic.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    \n",
    "    # Draw hand connections\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.left_hand_landmarks, \n",
    "            mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "        )\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            results.right_hand_landmarks, \n",
    "            mp_holistic.HAND_CONNECTIONS, \n",
    "            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740729379.480586    3863 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740729379.502740    6402 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3.4-arch1.1), renderer: AMD Radeon 680M (radeonsi, rembrandt, LLVM 19.1.7, DRM 3.60, 6.13.4-arch1-1)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740729379.566135    6379 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.595316    6376 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.599770    6391 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.600916    6379 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.601106    6381 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.609240    6378 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.618172    6385 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729379.618201    6382 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729380.019060    6389 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/strix/miniforge3/envs/epics/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "face = (np.array([[res.x, res.y, res.z] \n",
    "                  for res in results.face_landmarks.landmark]).flatten() \n",
    "        if results.face_landmarks \n",
    "        else np.zeros(1404))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47908697,  0.31720701, -1.36713505, ...,  0.        ,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('0', result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load('0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and actions\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "actions = np.array(['Hello', 'Bye', 'Deaf', 'Thanks'])\n",
    "no_sequences = 30\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Values for Training and Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if data exists\n",
    "def check_existing_data(data_path, action, sequence_length):\n",
    "    missing_sequences = []\n",
    "    action_path = os.path.join(data_path, action)\n",
    "    for sequence in range(no_sequences):\n",
    "        sequence_path = os.path.join(action_path, str(sequence))\n",
    "        # Check if all frames for this sequence exist\n",
    "        if not os.path.exists(sequence_path) or \\\n",
    "           len(os.listdir(sequence_path)) < sequence_length:\n",
    "            missing_sequences.append(sequence)\n",
    "    return missing_sequences\n",
    "\n",
    "# Function to collect data\n",
    "def capture_data(actions, sequence_length, no_sequences):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp.solutions.holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for action in actions:\n",
    "            print(f\"Processing action: {action}\")\n",
    "            \n",
    "            # Check missing sequences\n",
    "            missing_sequences = check_existing_data(DATA_PATH, action, sequence_length)\n",
    "            if not missing_sequences:\n",
    "                print(f\"All data for '{action}' is already collected. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            for sequence in missing_sequences:\n",
    "                print(f\"Collecting data for action '{action}', sequence {sequence}\")\n",
    "                for frame_num in range(sequence_length):\n",
    "                    ret, frame = cap.read()\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "                    draw_styled_landmarks(image, results)\n",
    "\n",
    "                    # Display progress\n",
    "                    if frame_num == 0:\n",
    "                        cv2.putText(image, f'STARTING COLLECTION for {action} (Seq: {sequence})', (15, 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(500)\n",
    "                    else:\n",
    "                        cv2.putText(image, f'Collecting frames for {action} (Seq: {sequence})', (15, 50),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "                    # Extract and save keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    sequence_path = os.path.join(DATA_PATH, action, str(sequence))\n",
    "                    os.makedirs(sequence_path, exist_ok=True)\n",
    "                    np.save(os.path.join(sequence_path, str(frame_num)), keypoints)\n",
    "\n",
    "                    # Display image\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                    # Exit on 'q'\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        return\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing action: Hello\n",
      "All data for 'Hello' is already collected. Skipping.\n",
      "Processing action: Bye\n",
      "All data for 'Bye' is already collected. Skipping.\n",
      "Processing action: Deaf\n",
      "All data for 'Deaf' is already collected. Skipping.\n",
      "Processing action: Thanks\n",
      "All data for 'Thanks' is already collected. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740729381.761291    3863 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740729381.764250    6459 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3.4-arch1.1), renderer: AMD Radeon 680M (radeonsi, rembrandt, LLVM 19.1.7, DRM 3.60, 6.13.4-arch1-1)\n",
      "W0000 00:00:1740729381.837574    6442 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.877700    6440 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.880322    6445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.880478    6450 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.882372    6452 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.887595    6448 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.893597    6445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740729381.900564    6453 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "capture_data(actions, sequence_length, no_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0, 'Bye': 1, 'Deaf': 2, 'Thanks': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 sequences\n"
     ]
    }
   ],
   "source": [
    "def load_gesture_data(DATA_PATH, actions, sequence_length):\n",
    "    sequences, labels = [], []\n",
    "    \n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        if not os.path.exists(action_path):\n",
    "            print(f\"Warning: Path {action_path} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        # Get valid sequence folders and sort them\n",
    "        sequence_folders = [f for f in os.listdir(action_path) \n",
    "                          if os.path.isdir(os.path.join(action_path, f))]\n",
    "        sequence_numbers = sorted([int(seq) for seq in sequence_folders])\n",
    "        \n",
    "        for sequence in sequence_numbers:\n",
    "            try:\n",
    "                window = []\n",
    "                sequence_path = os.path.join(action_path, str(sequence))\n",
    "                \n",
    "                # Check if all frame files exist\n",
    "                frames_exist = all(os.path.exists(os.path.join(sequence_path, f\"{frame_num}.npy\")) \n",
    "                                 for frame_num in range(sequence_length))\n",
    "                \n",
    "                if not frames_exist:\n",
    "                    print(f\"Skipping incomplete sequence: {sequence_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Load frame data\n",
    "                for frame_num in range(sequence_length):\n",
    "                    frame_path = os.path.join(sequence_path, f\"{frame_num}.npy\")\n",
    "                    res = np.load(frame_path)\n",
    "                    window.append(res)\n",
    "                    \n",
    "                sequences.append(window)\n",
    "                labels.append(label_map[action])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sequence {sequence} for action {action}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Load the data\n",
    "sequences, labels = load_gesture_data(DATA_PATH, actions, sequence_length)\n",
    "print(f\"Loaded {len(sequences)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 1662)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 1662)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Build and Train Multiple LSTM Neural Network Models\n",
    "\n",
    "We'll implement and compare 5 different model architectures with various regularization techniques to improve performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization, Bidirectional, GRU\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory if it doesn't exist\n",
    "log_dir = os.path.join('Logs')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join('Models')\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU usage by setting CUDA device to -1\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strix/miniforge3/envs/epics/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)),\n",
    "    LSTM(128, return_sequences=True, activation='relu'),\n",
    "    LSTM(64, return_sequences=False, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'), \n",
    "    Dense(actions.shape[0], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and number of classes\n",
    "input_shape = (30, 1662)  # 30 frames, 1662 features per frame\n",
    "num_classes = len(actions)\n",
    "\n",
    "# Train all models\n",
    "models = {\n",
    "    'LSTM_Dropout': create_model_1,\n",
    "    'BiLSTM_BatchNorm': create_model_2,\n",
    "    'GRU_L2Reg': create_model_3,\n",
    "    'LSTM_Residual': create_model_4,\n",
    "    'Hybrid_LSTM_GRU': create_model_5\n",
    "}\n",
    "\n",
    "# Store test accuracies for comparison\n",
    "test_accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model separately\n",
    "for model_name, model_creator in models.items():\n",
    "    model, history, test_acc = train_and_evaluate_model(\n",
    "        model_creator, model_name, input_shape, num_classes, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    test_accuracies[model_name] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Visualization\n",
    "\n",
    "Let's compare the performance of all 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history for multiple models\n",
    "def plot_training_comparison(model_histories, metric='categorical_accuracy'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name, history in model_histories.items():\n",
    "        # Plot training metric\n",
    "        plt.plot(history.history[metric], label=f'{model_name} (train)')\n",
    "        # Plot validation metric\n",
    "        plt.plot(history.history[f'val_{metric}'], linestyle='--', label=f'{model_name} (val)')\n",
    "    \n",
    "    plt.title(f'Model Comparison - {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plot_training_comparison(model_histories, 'categorical_accuracy')\n",
    "\n",
    "# Plot loss comparison\n",
    "plot_training_comparison(model_histories, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of model performance\n",
    "performance_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    history = model_histories[model_name]\n",
    "    \n",
    "    # Get the best validation accuracy and corresponding epoch\n",
    "    best_val_epoch = np.argmax(history.history['val_categorical_accuracy'])\n",
    "    best_val_acc = history.history['val_categorical_accuracy'][best_val_epoch]\n",
    "    \n",
    "    # Get the final training accuracy\n",
    "    final_train_acc = history.history['categorical_accuracy'][-1]\n",
    "    \n",
    "    # Get test accuracy\n",
    "    test_acc = test_accuracies[model_name]\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = model_training_times[model_name]\n",
    "    \n",
    "    # Calculate overfitting (difference between training and validation)\n",
    "    overfitting = final_train_acc - history.history['val_categorical_accuracy'][-1]\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Model': model_name,\n",
    "        'Best Val Accuracy': best_val_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Overfitting': overfitting\n",
    "    })\n",
    "\n",
    "# Create and display DataFrame\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart of test accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(performance_df['Model'], performance_df['Test Accuracy'], color='skyblue')\n",
    "\n",
    "# Add data labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Model Comparison - Test Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Best Model\n",
    "\n",
    "Let's save the best performing model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model based on test accuracy\n",
    "best_model_name = performance_df.iloc[0]['Model']\n",
    "print(f\"Best model based on test accuracy: {best_model_name}\")\n",
    "\n",
    "# Load the best model (the one saved during training)\n",
    "best_model_path = os.path.join(models_dir, f'{best_model_name}.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Save it with a descriptive name\n",
    "best_model.save('best_sign_language_model.keras')\n",
    "print(f\"Best model saved as 'best_sign_language_model.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "# plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training history for multiple models\n",
    "def plot_training_comparison(model_histories, metric='categorical_accuracy'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for model_name, history in model_histories.items():\n",
    "        # Plot training metric\n",
    "        plt.plot(history.history[metric], label=f'{model_name} (train)')\n",
    "        # Plot validation metric\n",
    "        plt.plot(history.history[f'val_{metric}'], linestyle='--', label=f'{model_name} (val)')\n",
    "    \n",
    "    plt.title(f'Model Comparison - {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plot_training_comparison(model_histories, 'categorical_accuracy')\n",
    "\n",
    "# Plot loss comparison\n",
    "plot_training_comparison(model_histories, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of model performance\n",
    "performance_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    history = model_histories[model_name]\n",
    "    \n",
    "    # Get the best validation accuracy and corresponding epoch\n",
    "    best_val_epoch = np.argmax(history.history['val_categorical_accuracy'])\n",
    "    best_val_acc = history.history['val_categorical_accuracy'][best_val_epoch]\n",
    "    \n",
    "    # Get the final training accuracy\n",
    "    final_train_acc = history.history['categorical_accuracy'][-1]\n",
    "    \n",
    "    # Get test accuracy\n",
    "    test_acc = test_accuracies[model_name]\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = model_training_times[model_name]\n",
    "    \n",
    "    # Calculate overfitting (difference between training and validation)\n",
    "    overfitting = final_train_acc - history.history['val_categorical_accuracy'][-1]\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Model': model_name,\n",
    "        'Best Val Accuracy': best_val_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Overfitting': overfitting\n",
    "    })\n",
    "\n",
    "# Create and display DataFrame\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart of test accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(performance_df['Model'], performance_df['Test Accuracy'], color='skyblue')\n",
    "\n",
    "# Add data labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Model Comparison - Test Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Best Model\n",
    "\n",
    "Let's save the best performing model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model based on test accuracy\n",
    "best_model_name = performance_df.iloc[0]['Model']\n",
    "print(f\"Best model based on test accuracy: {best_model_name}\")\n",
    "\n",
    "# Load the best model (the one saved during training)\n",
    "best_model_path = os.path.join(models_dir, f'{best_model_name}.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "# Save it with a descriptive name\n",
    "best_model.save('best_sign_language_model.keras')\n",
    "print(f\"Best model saved as 'best_sign_language_model.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Test Best Model in Real Time\n",
    "\n",
    "Let's test our best model in a real-time setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp_holistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Set mediapipe model \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmp_holistic\u001b[49m\u001b[38;5;241m.\u001b[39mHolistic(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m holistic:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# Read feed\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mp_holistic' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "# Load the best model for real-time testing\n",
    "model = best_model\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
